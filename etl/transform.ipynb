{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676a1e8e-208a-49ee-bad5-d0c06f119b78",
   "metadata": {},
   "source": [
    "# Notebook Markdown Section\n",
    "# ETL – Transform Phase for Instagram Analytics\n",
    "\n",
    "This section focuses on the **Transform** step of the ETL pipeline, where cleaned Instagram data is structured into **dimension and fact tables** suitable for analysis and Business Intelligence reporting.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Time Dimension (`time_dim`)\n",
    "- Extracts temporal information from the `upload_date`.\n",
    "- Adds:\n",
    "  - `year`\n",
    "  - `month`\n",
    "  - `day`\n",
    "- Each unique date in the dataset becomes a row in the time dimension.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Content Dimension (`content_dim`)\n",
    "- Captures characteristics of Instagram posts:\n",
    "  - `media_type` (e.g., Photo, Video)\n",
    "  - `content_category` (e.g., Lifestyle, Food)\n",
    "  - `caption_length`\n",
    "  - `hashtags_count`\n",
    "- Assigns a unique `content_id` to each distinct combination of attributes.\n",
    "- Serves as a **lookup table** for the fact table.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Fact Table (`instagram_fact`)\n",
    "- Combines the cleaned data with the content dimension.\n",
    "- Includes:\n",
    "  - Post metrics: `likes`, `comments`, `shares`, `saves`\n",
    "  - Reach and engagement: `reach`, `impressions`, `engagement_rate`\n",
    "  - Followers gained and traffic source\n",
    "  - Foreign key: `content_id` linking to the content dimension\n",
    "- Each row corresponds to a single Instagram post and its performance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Output\n",
    "- Saves transformed tables as CSV files in the `data/` folder:\n",
    "  - `time_dim.csv`\n",
    "  - `content_dim.csv`\n",
    "  - `instagram_fact.csv`\n",
    "\n",
    "---\n",
    "\n",
    "This structure allows for efficient **BI queries**, analytics, and dashboard creation by separating **dimensions** (time, content) from **facts** (post performance metrics).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3622c0-3b97-49a2-8cb1-064ef0c1fa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape: (29999, 15)\n",
      "Missing values before cleaning:\n",
      "post_id             0\n",
      "upload_date         0\n",
      "media_type          0\n",
      "likes               0\n",
      "comments            0\n",
      "shares              0\n",
      "saves               0\n",
      "reach               0\n",
      "impressions         0\n",
      "caption_length      0\n",
      "hashtags_count      0\n",
      "followers_gained    0\n",
      "traffic_source      0\n",
      "engagement_rate     0\n",
      "content_category    0\n",
      "dtype: int64\n",
      "Missing values after cleaning:\n",
      "post_id             0\n",
      "upload_date         0\n",
      "media_type          0\n",
      "likes               0\n",
      "comments            0\n",
      "shares              0\n",
      "saves               0\n",
      "reach               0\n",
      "impressions         0\n",
      "caption_length      0\n",
      "hashtags_count      0\n",
      "followers_gained    0\n",
      "traffic_source      0\n",
      "engagement_rate     0\n",
      "content_category    0\n",
      "dtype: int64\n",
      "C:\\Users\\malek\\OneDrive\\Desktop\\BI Project\\notebooks\n",
      "Cleaned data saved\n",
      "Transformation completed.\n",
      "Data validation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Transform_Data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "def clean_data():\n",
    "    # Load raw data\n",
    "    df = pd.read_csv(\"../data/staging/Instagram_Analytics.csv\")\n",
    "    print(\"Initial dataset shape:\", df.shape)\n",
    "    # ---- Missing Values ----\n",
    "    print(\"Missing values before cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "    # Categorical columns → mode\n",
    "    categorical_cols = ['media_type', 'traffic_source', 'content_category']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    # Numerical columns → median\n",
    "    numerical_cols = [\n",
    "        'likes', 'comments', 'shares', 'saves',\n",
    "        'reach', 'impressions', 'followers_gained',\n",
    "        'caption_length', 'hashtags_count', 'engagement_rate']\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    # ---- Data Type Fixing ----\n",
    "    df['upload_date'] = pd.to_datetime(df['upload_date'], errors='coerce')\n",
    "    # Remove rows with invalid dates\n",
    "    df = df.dropna(subset=['upload_date'])\n",
    "    # ---- Standardization ----\n",
    "    df['media_type'] = df['media_type'].str.title().str.strip()\n",
    "    df['traffic_source'] = df['traffic_source'].str.title().str.strip()\n",
    "    df['content_category'] = df['content_category'].str.title().str.strip()\n",
    "    # ---- Remove Duplicates ----\n",
    "    df = df.drop_duplicates(subset=['post_id'])\n",
    "    print(\"Missing values after cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "    import os\n",
    "    print(os.getcwd())\n",
    "    # Save cleaned data\n",
    "    df.to_csv(f'{processed_dir}/Instagram_Analytics_clean.csv', index=False)\n",
    "    print(\"Cleaned data saved\")\n",
    "def transform_data():\n",
    "    df = pd.read_csv(f'{processed_dir}/Instagram_Analytics_clean.csv')\n",
    "    # ---- Time Dimension ----\n",
    "    time_dim = df[['upload_date']].drop_duplicates()\n",
    "    time_dim['year'] = pd.to_datetime(time_dim['upload_date']).dt.year\n",
    "    time_dim['month'] = pd.to_datetime(time_dim['upload_date']).dt.month\n",
    "    time_dim['day'] = pd.to_datetime(time_dim['upload_date']).dt.day\n",
    "     # ---- Content Dimension ----\n",
    "    content_dim = df[[\n",
    "        'media_type', 'content_category',\n",
    "        'caption_length', 'hashtags_count'\n",
    "    ]].drop_duplicates().reset_index(drop=True)\n",
    "    content_dim['content_id'] = content_dim.index + 1\n",
    "\n",
    "    # ---- Fact Table ----\n",
    "    instagram_fact = df.merge(\n",
    "        content_dim,\n",
    "        on=['media_type', 'content_category', 'caption_length', 'hashtags_count'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    instagram_fact = instagram_fact[[\n",
    "        'post_id',\n",
    "        'upload_date',\n",
    "        'content_id',\n",
    "        'likes', 'comments', 'shares', 'saves',\n",
    "        'reach', 'impressions',\n",
    "        'followers_gained',\n",
    "        'engagement_rate',\n",
    "        'traffic_source']]\n",
    " # ---- Save Transformed Data ----\n",
    "    time_dim.to_csv(f'{processed_dir}/time_dim.csv', index=False)\n",
    "    content_dim.to_csv(f'{processed_dir}/content_dim.csv', index=False)\n",
    "    instagram_fact.to_csv(f'{processed_dir}/instagram_fact.csv', index=False)\n",
    "\n",
    "    print(\"Transformation completed.\")\n",
    "def validate_data():\n",
    "    time_dim = pd.read_csv(f'{processed_dir}/time_dim.csv')\n",
    "    content_dim = pd.read_csv(f'{processed_dir}/content_dim.csv')\n",
    "    fact = pd.read_csv(f'{processed_dir}/instagram_fact.csv')\n",
    "\n",
    "    # ---- Missing Values ----\n",
    "    assert time_dim.isnull().sum().sum() == 0, \"Missing values in Time_Dim\"\n",
    "    assert content_dim.isnull().sum().sum() == 0, \"Missing values in Content_Dim\"\n",
    "    assert fact.isnull().sum().sum() == 0, \"Missing values in Fact table\"\n",
    "\n",
    "    # ---- Key Integrity ----\n",
    "    assert fact['content_id'].isin(content_dim['content_id']).all(), \\\n",
    "        \"Invalid content_id in fact table\"\n",
    "\n",
    "    # ---- Numeric Validation ----\n",
    "    numeric_checks = ['likes', 'comments', 'shares', 'saves', 'reach', 'impressions']\n",
    "    for col in numeric_checks:\n",
    "        assert (fact[col] >= 0).all(), f\"Negative values found in {col}\"\n",
    "    print(\"Data validation completed successfully.\")\n",
    "def main():\n",
    "    clean_data()\n",
    "    transform_data()\n",
    "    validate_data()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "                                       \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfba831-a192-4f81-bf66-c444a118ed20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d7bfc-5836-4f9d-9537-7a937fb9fca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
